{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 09. Exercise 04\n",
                "# Pipelines and OOP"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Запуск контейнера\n",
                "\n",
                "docker run -d \\\\\n",
                "  --platform linux/amd64 \\\\\n",
                "  -p 8888:8888 \\\\\n",
                "  -v $(pwd):/home/jovyan/work \\\\\n",
                "  --name sklearn \\\\\n",
                "  jupyter/scipy-notebook:python-3.8 \\\\\n",
                "  bash -c \"pip install scikit-learn==0.23.1 tqdm==4.46.1 && start-notebook.sh --NotebookApp.token=''\"\n",
                "\n",
                "#### и выбираем правильный kernel в vscode на localhost (который отдает докер)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 330,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Python версия: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n",
                        "[GCC 10.3.0]\n",
                        "scikit-learn версия: 0.23.1\n",
                        "pandas версия: 1.5.0\n",
                        "numpy версия: 1.23.3\n",
                        "tqdm версия: 4.64.1\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "print(\"Python версия:\", sys.version)\n",
                "\n",
                "import sklearn\n",
                "print(\"scikit-learn версия:\", sklearn.__version__)\n",
                "\n",
                "import pandas as pd\n",
                "print(\"pandas версия:\", pd.__version__)\n",
                "\n",
                "import numpy as np\n",
                "print(\"numpy версия:\", np.__version__)\n",
                "\n",
                "import tqdm\n",
                "print(\"tqdm версия:\", tqdm.__version__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 331,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from datetime import datetime\n",
                "import pickle\n",
                "import os\n",
                "\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# import warnings\n",
                "# warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Preprocessing pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Create three custom transformers, the first two out of which will be used within a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
                "\n",
                "1. `FeatureExtractor()` class:\n",
                " - Takes a dataframe with `uid`, `labname`, `numTrials`, `timestamp` from the file [`checker_submits.csv`](https://drive.google.com/file/d/14voc4fNJZiLEFaZyd8nEG-lQt5JjatYw/view?usp=sharing).\n",
                " - Extracts `hour` from `timestamp`.\n",
                " - Extracts `weekday` from `timestamp` (numbers).\n",
                " - Drops the `timestamp` column.\n",
                " - Returns the new dataframe.\n",
                "\n",
                "\n",
                "2. `MyOneHotEncoder()` class:\n",
                " - Takes the dataframe from the result of the previous transformation and the name of the target column.\n",
                " - Identifies all the categorical features and transforms them with `OneHotEncoder()`. If the target column is categorical too, then the transformation should not apply to it.\n",
                " - Drops the initial categorical features.\n",
                " - Returns the dataframe with the features and the series with the target column.\n",
                "\n",
                "\n",
                "3. `TrainValidationTest()` class:\n",
                " - Takes `X` and `y`.\n",
                " - Returns `X_train`, `X_valid`, `X_test`, `y_train`, `y_valid`, `y_test` (`test_size=0.2`, `random_state=21`, `stratified`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 332,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
                "    \"\"\"\n",
                "    Кастомный трансформер для извлечения признаков из временных меток.\n",
                "    \n",
                "    Входные данные: DataFrame с колонками uid, labname, numTrials, timestamp\n",
                "    Выходные данные: DataFrame с дополнительными колонками hour, weekday (без timestamp)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        pass\n",
                "    \n",
                "    def fit(self, X, y=None):\n",
                "        \"\"\"Метод fit - обязательный для sklearn трансформеров\"\"\"\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X):\n",
                "        \"\"\"\n",
                "        Основная логика трансформации:\n",
                "        - Извлекает час из timestamp\n",
                "        - Извлекает день недели из timestamp (числа)\n",
                "        - Удаляет исходную колонку timestamp\n",
                "        \"\"\"\n",
                "        X_copy = X.copy()\n",
                "        \n",
                "        # Преобразуем timestamp в datetime если это строки\n",
                "        if X_copy['timestamp'].dtype == 'object':\n",
                "            X_copy['timestamp'] = pd.to_datetime(X_copy['timestamp'])\n",
                "        \n",
                "        # Извлекаем час (0-23)\n",
                "        X_copy['hour'] = X_copy['timestamp'].dt.hour\n",
                "        \n",
                "        # Извлекаем день недели (0=понедельник, 6=воскресенье)\n",
                "        X_copy['weekday'] = X_copy['timestamp'].dt.dayofweek\n",
                "        \n",
                "        # Удаляем исходную колонку timestamp\n",
                "        X_copy = X_copy.drop('timestamp', axis=1)\n",
                "        \n",
                "        return X_copy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 333,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MyOneHotEncoder(BaseEstimator, TransformerMixin):\n",
                "    \"\"\"\n",
                "    ИСПРАВЛЕННАЯ ВЕРСИЯ - правильно обрабатывает категориальную целевую переменную\n",
                "    \n",
                "    Автоматически определяет категориальные колонки и применяет OneHotEncoder,\n",
                "    исключая целевую переменную из трансформации.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, target_column):\n",
                "        self.target_column = target_column\n",
                "        self.categorical_columns = []\n",
                "        self.encoder = None\n",
                "        self.encoded_feature_names = []\n",
                "    \n",
                "    def fit(self, X, y=None):\n",
                "        \"\"\"\n",
                "        Определяет категориальные колонки и обучает OneHotEncoder\n",
                "        ИСКЛЮЧАЯ целевую переменную даже если она категориальная\n",
                "        \"\"\"\n",
                "        # Определяем категориальные колонки (исключая целевую)\n",
                "        self.categorical_columns = []\n",
                "        for col in X.columns:\n",
                "            if col != self.target_column:  # исключаем целевую колонку\n",
                "                # Проверяем является ли колонка категориальной\n",
                "                if (X[col].dtype == 'object' or \n",
                "                    (X[col].dtype in ['int64', 'float64'] and X[col].nunique() <= 50)):\n",
                "                    self.categorical_columns.append(col)\n",
                "        \n",
                "        if self.categorical_columns:\n",
                "            # Обучаем OneHotEncoder только на признаках\n",
                "            self.encoder = OneHotEncoder(sparse=False, drop='first')\n",
                "            self.encoder.fit(X[self.categorical_columns])\n",
                "            \n",
                "            # Сохраняем названия новых признаков\n",
                "            self.encoded_feature_names = self.encoder.get_feature_names(\n",
                "                self.categorical_columns\n",
                "            )\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def transform(self, X):\n",
                "        \"\"\"\n",
                "        Применяет one-hot кодирование ТОЛЬКО к признакам, НЕ к целевой переменной\n",
                "        Возвращает X (признаки) и y (целевая переменная) отдельно\n",
                "        \"\"\"\n",
                "        X_copy = X.copy()\n",
                "        \n",
                "        # Извлекаем целевую переменную БЕЗ трансформации\n",
                "        # даже если она категориальная\n",
                "        y = X_copy[self.target_column].copy()\n",
                "        \n",
                "        if self.categorical_columns and self.encoder is not None:\n",
                "            # Применяем one-hot кодирование только к признакам\n",
                "            encoded_data = self.encoder.transform(X_copy[self.categorical_columns])\n",
                "            \n",
                "            # Создаем DataFrame с закодированными признаками\n",
                "            encoded_df = pd.DataFrame(\n",
                "                encoded_data, \n",
                "                columns=self.encoded_feature_names,\n",
                "                index=X_copy.index\n",
                "            )\n",
                "            \n",
                "            # Удаляем исходные категориальные колонки\n",
                "            X_copy = X_copy.drop(self.categorical_columns, axis=1)\n",
                "            \n",
                "            # Добавляем закодированные признаки\n",
                "            X_copy = pd.concat([X_copy, encoded_df], axis=1)\n",
                "        \n",
                "        # Удаляем целевую колонку из признаков\n",
                "        X_copy = X_copy.drop(self.target_column, axis=1)\n",
                "        \n",
                "        return X_copy, y"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 334,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TrainValidationTest:\n",
                "    \"\"\"\n",
                "    Класс для разделения данных на тренировочную, валидационную и тестовую выборки.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        pass\n",
                "    \n",
                "    def split(self, X, y, test_size=0.2, random_state=21):\n",
                "        \"\"\"\n",
                "        Разделяет данные на train/validation/test с стратификацией\n",
                "        Возвращает X_train, X_valid, X_test, y_train, y_valid, y_test\n",
                "        \"\"\"\n",
                "        # Сначала отделяем test set\n",
                "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
                "            X, y, test_size=test_size, random_state=random_state, \n",
                "            stratify=y\n",
                "        )\n",
                "        \n",
                "        # Затем из оставшихся данных делаем train/validation\n",
                "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
                "            X_temp, y_temp, test_size=test_size, random_state=random_state,\n",
                "            stratify=y_temp\n",
                "        )\n",
                "        \n",
                "        return X_train, X_valid, X_test, y_train, y_valid, y_test"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model selection pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`ModelSelection()` class\n",
                "\n",
                " - Takes a list of `GridSearchCV` instances and a dict where the keys are the indexes from that list and the values are the names of the models, the example is below in the reverse order (from high-level to low-level perspective):\n",
                "\n",
                "```\n",
                "ModelSelection(grids, grid_dict)\n",
                "\n",
                "grids = [gs_svm, gs_tree, gs_rf]\n",
                "\n",
                "gs_svm = GridSearchCV(estimator=svm, param_grid=svm_params, scoring='accuracy', cv=2, n_jobs=jobs), where jobs you can specify by yourself\n",
                "\n",
                "svm_params = [{'kernel':('linear', 'rbf', 'sigmoid'), 'C':[0.01, 0.1, 1, 1.5, 5, 10], 'gamma': ['scale', 'auto'], 'class_weight':('balanced', None), 'random_state':[21], 'probability':[True]}]\n",
                "```\n",
                "\n",
                " - Method `choose()` takes `X_train`, `y_train`, `X_valid`, `y_valid` and returns the name of the best classifier among all the models on the validation set\n",
                " - Method `best_results()` returns a dataframe with the columns `model`, `params`, `valid_score` where the rows are the best models within each class of models.\n",
                "\n",
                "```\n",
                "model\tparams\tvalid_score\n",
                "0\tSVM\t{'C': 10, 'class_weight': None, 'gamma': 'auto...\t0.772727\n",
                "1\tDecision Tree\t{'class_weight': 'balanced', 'criterion': 'gin...\t0.801484\n",
                "2\tRandom Forest\t{'class_weight': None, 'criterion': 'entropy',...\t0.855288\n",
                "```\n",
                "\n",
                " - When you iterate through the parameters of a model class, print the name of that class and show the progress using `tqdm.notebook`, in the end of the cycle print the best model of that class.\n",
                "\n",
                "```\n",
                "Estimator: SVM\n",
                "100%\n",
                "125/125 [01:32<00:00, 1.36it/s]\n",
                "Best params: {'C': 10, 'class_weight': None, 'gamma': 'auto', 'kernel': 'rbf', 'probability': True, 'random_state': 21}\n",
                "Best training accuracy: 0.773\n",
                "Validation set accuracy score for best params: 0.878 \n",
                "\n",
                "Estimator: Decision Tree\n",
                "100%\n",
                "57/57 [01:07<00:00, 1.22it/s]\n",
                "Best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 21, 'random_state': 21}\n",
                "Best training accuracy: 0.801\n",
                "Validation set accuracy score for best params: 0.867 \n",
                "\n",
                "Estimator: Random Forest\n",
                "100%\n",
                "284/284 [06:47<00:00, 1.13s/it]\n",
                "Best params: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 22, 'n_estimators': 50, 'random_state': 21}\n",
                "Best training accuracy: 0.855\n",
                "Validation set accuracy score for best params: 0.907 \n",
                "\n",
                "Classifier with best validation set accuracy: Random Forest\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 335,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ModelSelection:\n",
                "    \"\"\"    \n",
                "    Класс для автоматического выбора лучшей модели среди нескольких алгоритмов.\n",
                "    Использует GridSearchCV для поиска лучших гиперпараметров каждого алгоритма,\n",
                "    затем сравнивает их производительность на валидационной выборке.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, grids, grid_dict):\n",
                "        \"\"\"\n",
                "        grids: список GridSearchCV объектов\n",
                "        grid_dict: словарь {индекс: название_модели}\n",
                "        \"\"\"\n",
                "        self.grids = grids\n",
                "        self.grid_dict = grid_dict\n",
                "        self.best_estimators = {}\n",
                "        self.best_scores = {}\n",
                "        self.results_df = None\n",
                "    \n",
                "    def choose(self, X_train, y_train, X_valid, y_valid):\n",
                "        \"\"\"\n",
                "        Обучает все модели и выбирает лучшую по accuracy на валидации\n",
                "        \"\"\"\n",
                "        results = []\n",
                "        best_model_name = None\n",
                "        best_valid_score = -1\n",
                "        \n",
                "        for i, grid in enumerate(self.grids):\n",
                "            model_name = self.grid_dict[i]\n",
                "            print(f\"Estimator: {model_name}\")\n",
                "            \n",
                "            # Подсчитываем общее количество комбинаций\n",
                "            total_combinations = 1\n",
                "            for param_name, param_values in grid.param_grid.items():\n",
                "                total_combinations *= len(param_values)\n",
                "            \n",
                "            with tqdm(total=total_combinations, \n",
                "                     bar_format='{percentage:3.0f}%\\n{n}/{total} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:\n",
                "                grid.fit(X_train, y_train)\n",
                "                pbar.update(total_combinations)\n",
                "            \n",
                "            # Получаем результаты\n",
                "            best_estimator = grid.best_estimator_\n",
                "            best_params = grid.best_params_\n",
                "            best_train_score = grid.best_score_\n",
                "            \n",
                "            # Оцениваем на валидации\n",
                "            valid_score = accuracy_score(y_valid, best_estimator.predict(X_valid))\n",
                "            \n",
                "            # Сохраняем результаты\n",
                "            self.best_estimators[model_name] = best_estimator\n",
                "            self.best_scores[model_name] = valid_score\n",
                "            \n",
                "            results.append({\n",
                "                'model': model_name,\n",
                "                'params': best_params,\n",
                "                'valid_score': valid_score\n",
                "            })\n",
                "            \n",
                "            # Отслеживаем лучшую модель\n",
                "            if valid_score > best_valid_score:\n",
                "                best_valid_score = valid_score\n",
                "                best_model_name = model_name\n",
                "            \n",
                "            print(f\"Best params: {best_params}\")\n",
                "            print(f\"Best training accuracy: {best_train_score:.3f}\")\n",
                "            print(f\"Validation set accuracy score for best params: {valid_score:.3f} \")\n",
                "            print()  # Пустая строка между моделями\n",
                "        \n",
                "        # Сохраняем результаты в DataFrame\n",
                "        self.results_df = pd.DataFrame(results)\n",
                "        \n",
                "        print(f\"Classifier with best validation set accuracy: {best_model_name}\")\n",
                "        \n",
                "        return best_model_name\n",
                "    \n",
                "    def best_results(self):\n",
                "        \"\"\"\n",
                "        Возвращает DataFrame с результатами всех моделей\n",
                "        \"\"\"\n",
                "        if self.results_df is not None:\n",
                "            # Форматируем params как строки (обрезанные)\n",
                "            formatted_results = self.results_df.copy()\n",
                "            formatted_results['params'] = formatted_results['params'].apply(\n",
                "                lambda x: str(x)[:50] + '...' if len(str(x)) > 50 else str(x)\n",
                "            )\n",
                "            return formatted_results[['model', 'params', 'valid_score']]\n",
                "        else:\n",
                "            print(\"Run choose() method first!\")\n",
                "            return None\n",
                "    \n",
                "    def get_best_estimator(self, model_name):\n",
                "        \"\"\"\n",
                "        Возвращает лучший estimator для указанной модели\n",
                "        \"\"\"\n",
                "        return self.best_estimators.get(model_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Finalization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`Finalize()` class\n",
                " - Takes an estimator.\n",
                " - Method `final_score()` takes `X_train`, `y_train`, `X_test`, `y_test` and returns the accuracy of the model as in the example below:\n",
                "```\n",
                "final.final_score(X_train, y_train, X_test, y_test)\n",
                "Accuracy of the final model is 0.908284023668639\n",
                "```\n",
                " - Method `save_model()` takes a path, saves the model to this path and prints that the model was successfully saved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 336,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Finalize:\n",
                "    \"\"\"    \n",
                "    Класс для финализации модели - получения финального результата и сохранения.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, estimator):\n",
                "        self.estimator = estimator\n",
                "    \n",
                "    def final_score(self, X_train, y_train, X_test, y_test):\n",
                "        \"\"\"\n",
                "        Обучает модель на всех тренировочных данных и оценивает на тесте\n",
                "        \"\"\"\n",
                "        # Обучаем на полных тренировочных данных\n",
                "        self.estimator.fit(X_train, y_train)\n",
                "        \n",
                "        # Предсказываем на тестовой выборке\n",
                "        y_pred = self.estimator.predict(X_test)\n",
                "        \n",
                "        # Вычисляем accuracy\n",
                "        accuracy = accuracy_score(y_test, y_pred)\n",
                "        \n",
                "        print(f\"Accuracy of the final model is {accuracy}\")\n",
                "        \n",
                "        return accuracy\n",
                "    \n",
                "    def save_model(self, path):\n",
                "        \"\"\"\n",
                "        Сохраняет модель в указанный путь\n",
                "        \"\"\"\n",
                "        # Создаем директорию если не существует\n",
                "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
                "        \n",
                "        with open(path, 'wb') as f:\n",
                "            pickle.dump(self.estimator, f)\n",
                "        \n",
                "        print(f\"Model successfully saved to {path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Main program"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1. Load the data from the file (****name of file****).\n",
                "2. Create the preprocessing pipeline that consists of two custom transformers: `FeatureExtractor()` and `MyOneHotEncoder()`:\n",
                "```\n",
                "preprocessing = Pipeline([('feature_extractor', FeatureExtractor()), ('onehot_encoder', MyOneHotEncoder('dayofweek'))])\n",
                "```\n",
                "3. Use that pipeline and its method `fit_transform()` on the initial dataset.\n",
                "```\n",
                "data = preprocessing.fit_transform(df)\n",
                "```\n",
                "4. Get `X_train`, `X_valid`, `X_test`, `y_train`, `y_valid`, `y_test` using `TrainValidationTest()` and the result of the pipeline.\n",
                "5. Create an instance of `ModelSelection()`, use the method `choose()` applying it to the models that you want and parameters that you want, get the dataframe of the best results.\n",
                "6. create an instance of `Finalize()` with your best model, use method `final_score()` and save the model in the format: `name_of_the_model_{accuracy on test dataset}.sav`.\n",
                "\n",
                "That is it, congrats!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Загрузка данных"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 337,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Загрузка данных... ===\n",
                        "\n",
                        "Размер данных: (1686, 4)\n",
                        "Колонки: ['uid', 'labname', 'numTrials', 'timestamp']\n",
                        "\n",
                        "Первые несколько строк:\n",
                        "      uid   labname  numTrials                   timestamp\n",
                        "0  user_4  project1          1  2020-04-17 05:19:02.744528\n",
                        "1  user_4  project1          2  2020-04-17 05:22:45.549397\n",
                        "2  user_4  project1          3  2020-04-17 05:34:24.422370\n",
                        "3  user_4  project1          4  2020-04-17 05:43:27.773992\n",
                        "4  user_4  project1          5  2020-04-17 05:46:32.275104\n",
                        "\n",
                        "Общая информация:\n",
                        "- Уникальных пользователей: 30\n",
                        "- Уникальных лабораторных: 11\n",
                        "- Период данных: 2020-04-17 05:19:02.744528 - 2020-05-21 20:37:00.290491\n",
                        "CPU times: user 42.5 ms, sys: 7.03 ms, total: 49.5 ms\n",
                        "Wall time: 47.6 ms\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "# 1. Load the data from the file checker_submits.csv\n",
                "print(\"=== Загрузка данных... ===\\n\")\n",
                "df = pd.read_csv('work/src/data/checker_submits.csv')\n",
                "\n",
                "print(f\"Размер данных: {df.shape}\")\n",
                "print(f\"Колонки: {df.columns.tolist()}\")\n",
                "print(\"\\nПервые несколько строк:\")\n",
                "print(df.head())\n",
                "\n",
                "print(f\"\\nОбщая информация:\")\n",
                "print(f\"- Уникальных пользователей: {df['uid'].nunique()}\")\n",
                "print(f\"- Уникальных лабораторных: {df['labname'].nunique()}\")\n",
                "print(f\"- Период данных: {df['timestamp'].min()} - {df['timestamp'].max()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Создание и применение preprocessing pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 338,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Создание preprocessing pipeline... ===\n",
                        "\n",
                        "Pipeline создан:\n",
                        "  - feature_extractor: FeatureExtractor\n",
                        "  - onehot_encoder: MyOneHotEncoder\n"
                    ]
                }
            ],
            "source": [
                "# 2. Create the preprocessing pipeline that consists of \n",
                "# two custom transformers: `FeatureExtractor()` and `MyOneHotEncoder()`:\n",
                "print(\"=== Создание preprocessing pipeline... ===\\n\")\n",
                "\n",
                "preprocessing = Pipeline([\n",
                "    ('feature_extractor', FeatureExtractor()),\n",
                "    ('onehot_encoder', MyOneHotEncoder('weekday'))  # weekday как целевая переменная\n",
                "])\n",
                "\n",
                "print(\"Pipeline создан:\")\n",
                "for step_name, step_transformer in preprocessing.steps:\n",
                "    print(f\"  - {step_name}: {step_transformer.__class__.__name__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 339,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Применение preprocessing pipeline... ===\n",
                        "\n",
                        "Размер признаков после preprocessing: (1686, 61)\n",
                        "Размер целевой переменной: (1686,)\n",
                        "Колонки признаков: ['numTrials', 'uid_user_1', 'uid_user_10', 'uid_user_11', 'uid_user_12', 'uid_user_13', 'uid_user_14', 'uid_user_15', 'uid_user_16', 'uid_user_17', 'uid_user_18', 'uid_user_19', 'uid_user_2', 'uid_user_20', 'uid_user_21', 'uid_user_22', 'uid_user_23', 'uid_user_24', 'uid_user_25', 'uid_user_26', 'uid_user_27', 'uid_user_28', 'uid_user_29', 'uid_user_3', 'uid_user_30', 'uid_user_31', 'uid_user_4', 'uid_user_6', 'uid_user_7', 'uid_user_8', 'labname_lab02', 'labname_lab03', 'labname_lab03s', 'labname_lab05s', 'labname_laba04', 'labname_laba04s', 'labname_laba05', 'labname_laba06', 'labname_laba06s', 'labname_project1', 'hour_1', 'hour_3', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23']\n",
                        "\n",
                        "Первые несколько строк признаков:\n",
                        "   numTrials  uid_user_1  uid_user_10  uid_user_11  uid_user_12  uid_user_13  \\\n",
                        "0          1         0.0          0.0          0.0          0.0          0.0   \n",
                        "1          2         0.0          0.0          0.0          0.0          0.0   \n",
                        "2          3         0.0          0.0          0.0          0.0          0.0   \n",
                        "3          4         0.0          0.0          0.0          0.0          0.0   \n",
                        "4          5         0.0          0.0          0.0          0.0          0.0   \n",
                        "\n",
                        "   uid_user_14  uid_user_15  uid_user_16  uid_user_17  ...  hour_14  hour_15  \\\n",
                        "0          0.0          0.0          0.0          0.0  ...      0.0      0.0   \n",
                        "1          0.0          0.0          0.0          0.0  ...      0.0      0.0   \n",
                        "2          0.0          0.0          0.0          0.0  ...      0.0      0.0   \n",
                        "3          0.0          0.0          0.0          0.0  ...      0.0      0.0   \n",
                        "4          0.0          0.0          0.0          0.0  ...      0.0      0.0   \n",
                        "\n",
                        "   hour_16  hour_17  hour_18  hour_19  hour_20  hour_21  hour_22  hour_23  \n",
                        "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
                        "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
                        "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
                        "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
                        "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
                        "\n",
                        "[5 rows x 61 columns]\n",
                        "\n",
                        "Распределение целевой переменной (weekday):\n",
                        "Monday (day 0): 136 samples\n",
                        "Tuesday (day 1): 274 samples\n",
                        "Wednesday (day 2): 149 samples\n",
                        "Thursday (day 3): 396 samples\n",
                        "Friday (day 4): 104 samples\n",
                        "Saturday (day 5): 271 samples\n",
                        "Sunday (day 6): 356 samples\n",
                        "CPU times: user 110 ms, sys: 14.3 ms, total: 124 ms\n",
                        "Wall time: 118 ms\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "# 3. Use that pipeline and its method fit_transform() on the initial dataset\n",
                "print(\"=== Применение preprocessing pipeline... ===\\n\")\n",
                "\n",
                "X, y = preprocessing.fit_transform(df)\n",
                "\n",
                "print(f\"Размер признаков после preprocessing: {X.shape}\")\n",
                "print(f\"Размер целевой переменной: {y.shape}\")\n",
                "print(f\"Колонки признаков: {X.columns.tolist()}\")\n",
                "\n",
                "print(\"\\nПервые несколько строк признаков:\")\n",
                "print(X.head())\n",
                "\n",
                "print(\"\\nРаспределение целевой переменной (weekday):\")\n",
                "weekday_counts = y.value_counts().sort_index()\n",
                "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
                "for i, count in enumerate(weekday_counts):\n",
                "    print(f\"{weekdays[i]} (day {i}): {count} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Разделение данных"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 340,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Разделение данных на train/validation/test... ===\n",
                        "\n",
                        "Train set: 1078 samples\n",
                        "Validation set: 270 samples\n",
                        "Test set: 338 samples\n",
                        "CPU times: user 12.6 ms, sys: 3.02 ms, total: 15.6 ms\n",
                        "Wall time: 14.2 ms\n"
                    ]
                }
            ],
            "source": [
                "%%time \n",
                "\n",
                "# 4. Get `X_train`, `X_valid`, `X_test`, `y_train`, `y_valid`, `y_test` \n",
                "# using `TrainValidationTest()` and the result of the pipeline.\n",
                "print(\"=== Разделение данных на train/validation/test... ===\\n\")\n",
                "\n",
                "splitter = TrainValidationTest()\n",
                "X_train, X_valid, X_test, y_train, y_valid, y_test = splitter.split(X, y)\n",
                "\n",
                "print(f\"Train set: {X_train.shape[0]} samples\")\n",
                "print(f\"Validation set: {X_valid.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 341,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Распределение в train set:\n",
                        "Monday: 87\n",
                        "Tuesday: 175\n",
                        "Wednesday: 95\n",
                        "Thursday: 253\n",
                        "Friday: 66\n",
                        "Saturday: 174\n",
                        "Sunday: 228\n",
                        "\n",
                        "Распределение в validation set:\n",
                        "Monday: 22\n",
                        "Tuesday: 44\n",
                        "Wednesday: 24\n",
                        "Thursday: 63\n",
                        "Friday: 17\n",
                        "Saturday: 43\n",
                        "Sunday: 57\n",
                        "\n",
                        "Распределение в test set:\n",
                        "Monday: 27\n",
                        "Tuesday: 55\n",
                        "Wednesday: 30\n",
                        "Thursday: 80\n",
                        "Friday: 21\n",
                        "Saturday: 54\n",
                        "Sunday: 71\n"
                    ]
                }
            ],
            "source": [
                "print(\"Распределение в train set:\")\n",
                "train_dist = y_train.value_counts().sort_index()\n",
                "for i, count in enumerate(train_dist):\n",
                "    print(f\"{weekdays[i]}: {count}\")\n",
                "\n",
                "print(\"\\nРаспределение в validation set:\")\n",
                "valid_dist = y_valid.value_counts().sort_index()\n",
                "for i, count in enumerate(valid_dist):\n",
                "    print(f\"{weekdays[i]}: {count}\")\n",
                "\n",
                "print(\"\\nРаспределение в test set:\")\n",
                "test_dist = y_test.value_counts().sort_index()\n",
                "for i, count in enumerate(test_dist):\n",
                "    print(f\"{weekdays[i]}: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Настройка моделей для GridSearch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 342,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Настройка моделей для GridSearch... ===\n",
                        "\n",
                        "SVM: 72 комбинаций\n",
                        "Decision Tree: 28 комбинаций\n",
                        "Random Forest: 60 комбинаций\n",
                        "\n",
                        "Всего: 160 комбинаций\n"
                    ]
                }
            ],
            "source": [
                "# 5. Create an instance of `ModelSelection()`, use the method `choose()` \n",
                "# applying it to the models that you want and parameters that you want, \n",
                "# get the dataframe of the best results.\n",
                "print(\"=== Настройка моделей для GridSearch... ===\\n\")\n",
                "\n",
                "# Параметры для SVM (на основе лучших результатов из ex02/ex03)\n",
                "svm_params = {\n",
                "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
                "    'C': [0.01, 0.1, 1, 1.5, 5, 10],\n",
                "    'gamma': ['scale', 'auto'],\n",
                "    'class_weight': ['balanced', None],\n",
                "    'random_state': [21],\n",
                "    'probability': [True]\n",
                "}\n",
                "\n",
                "# Параметры для Decision Tree (на основе лучших результатов из ex02/ex03)\n",
                "tree_params = {\n",
                "    'criterion': ['gini', 'entropy'],\n",
                "    'max_depth': [10, 15, 20, 21, 22, 25, None],\n",
                "    'class_weight': ['balanced', None],\n",
                "    'random_state': [21]\n",
                "}\n",
                "\n",
                "# Параметры для Random Forest (на основе лучших результатов из ex02/ex03)\n",
                "rf_params = {\n",
                "    'n_estimators': [30, 50, 100],\n",
                "    'criterion': ['gini', 'entropy'],\n",
                "    'max_depth': [20, 21, 22, 25, None],\n",
                "    'class_weight': ['balanced', None],\n",
                "    'random_state': [21]\n",
                "}\n",
                "\n",
                "# Создание GridSearchCV объектов\n",
                "gs_svm = GridSearchCV(\n",
                "    estimator=SVC(),\n",
                "    param_grid=svm_params,\n",
                "    scoring='accuracy',\n",
                "    cv=2,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "gs_tree = GridSearchCV(\n",
                "    estimator=DecisionTreeClassifier(),\n",
                "    param_grid=tree_params,\n",
                "    scoring='accuracy',\n",
                "    cv=2,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "gs_rf = GridSearchCV(\n",
                "    estimator=RandomForestClassifier(),\n",
                "    param_grid=rf_params,\n",
                "    scoring='accuracy',\n",
                "    cv=2,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "grids = [gs_svm, gs_tree, gs_rf]\n",
                "grid_dict = {0: 'SVM', 1: 'Decision Tree', 2: 'Random Forest'}\n",
                "\n",
                "# Подсчитываем общее количество комбинаций\n",
                "svm_combinations = np.prod([len(v) for v in svm_params.values()])\n",
                "tree_combinations = np.prod([len(v) for v in tree_params.values()])\n",
                "rf_combinations = np.prod([len(v) for v in rf_params.values()])\n",
                "\n",
                "print(f\"SVM: {svm_combinations} комбинаций\")\n",
                "print(f\"Decision Tree: {tree_combinations} комбинаций\")\n",
                "print(f\"Random Forest: {rf_combinations} комбинаций\")\n",
                "print(f\"\\nВсего: {svm_combinations + tree_combinations + rf_combinations} комбинаций\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Выбор лучшей модели"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 343,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== ВЫБОР ЛУЧШЕЙ МОДЕЛИ ===\n",
                        "\n",
                        "Estimator: SVM\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d3cf4e02430847ffbc9c88a454111647",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%\n",
                            "0/72 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best params: {'C': 10, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf', 'probability': True, 'random_state': 21}\n",
                        "Best training accuracy: 0.733\n",
                        "Validation set accuracy score for best params: 0.815 \n",
                        "\n",
                        "Estimator: Decision Tree\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "195d4f80779148fe8c56b4ec8bb0457a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%\n",
                            "0/28 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best params: {'class_weight': None, 'criterion': 'gini', 'max_depth': 22, 'random_state': 21}\n",
                        "Best training accuracy: 0.781\n",
                        "Validation set accuracy score for best params: 0.867 \n",
                        "\n",
                        "Estimator: Random Forest\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3ec5adb22a3443b2803a6cbc8dcebb72",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%\n",
                            "0/60 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best params: {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'n_estimators': 100, 'random_state': 21}\n",
                        "Best training accuracy: 0.820\n",
                        "Validation set accuracy score for best params: 0.878 \n",
                        "\n",
                        "Classifier with best validation set accuracy: Random Forest\n",
                        "CPU times: user 2.44 s, sys: 557 ms, total: 3 s\n",
                        "Wall time: 39.3 s\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "# Model selection\n",
                "print(\"=== ВЫБОР ЛУЧШЕЙ МОДЕЛИ ===\\n\")\n",
                "\n",
                "model_selector = ModelSelection(grids, grid_dict)\n",
                "best_model_name = model_selector.choose(X_train, y_train, X_valid, y_valid)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 344,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== РЕЗУЛЬТАТЫ ВСЕХ МОДЕЛЕЙ ===\n",
                        "\n",
                        "        model                                                params  valid_score\n",
                        "          SVM {'C': 10, 'class_weight': 'balanced', 'gamma': 'au...     0.814815\n",
                        "Decision Tree {'class_weight': None, 'criterion': 'gini', 'max_d...     0.866667\n",
                        "Random Forest {'class_weight': None, 'criterion': 'gini', 'max_d...     0.877778\n"
                    ]
                }
            ],
            "source": [
                "print(\"=== РЕЗУЛЬТАТЫ ВСЕХ МОДЕЛЕЙ ===\\n\")\n",
                "results_df = model_selector.best_results()\n",
                "print(results_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Финализация и сохранение модели"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 345,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== ФИНАЛИЗАЦИЯ С RANDOM FOREST ===\n",
                        "\n",
                        "Лучшие параметры Random Forest:\n",
                        "  bootstrap: True\n",
                        "  ccp_alpha: 0.0\n",
                        "  class_weight: None\n",
                        "  criterion: gini\n",
                        "  max_depth: None\n",
                        "  max_features: auto\n",
                        "  max_leaf_nodes: None\n",
                        "  max_samples: None\n",
                        "  min_impurity_decrease: 0.0\n",
                        "  min_impurity_split: None\n",
                        "  min_samples_leaf: 1\n",
                        "  min_samples_split: 2\n",
                        "  min_weight_fraction_leaf: 0.0\n",
                        "  n_estimators: 100\n",
                        "  n_jobs: None\n",
                        "  oob_score: False\n",
                        "  random_state: 21\n",
                        "  verbose: 0\n",
                        "  warm_start: False\n"
                    ]
                }
            ],
            "source": [
                "# 6. create an instance of `Finalize()` with your best model, \n",
                "# use method `final_score()` and save the model in the format: \n",
                "# `name_of_the_model_{accuracy on test dataset}.sav`.\n",
                "print(f\"=== ФИНАЛИЗАЦИЯ С {best_model_name.upper()} ===\\n\")\n",
                "\n",
                "best_estimator = model_selector.get_best_estimator(best_model_name)\n",
                "print(f\"Лучшие параметры {best_model_name}:\")\n",
                "for param, value in best_estimator.get_params().items():\n",
                "    print(f\"  {param}: {value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 346,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Финальное обучение на 1348 образцах...\n",
                        "\n",
                        "Accuracy of the final model is 0.8964497041420119\n",
                        "\n",
                        "Сохранение модели как: work/src/ex04/model/random_forest_0.896450.model\n",
                        "Model successfully saved to work/src/ex04/model/random_forest_0.896450.model\n",
                        "CPU times: user 332 ms, sys: 22.3 ms, total: 354 ms\n",
                        "Wall time: 356 ms\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "# Объединяем train и validation для финального обучения\n",
                "X_train_full = pd.concat([X_train, X_valid], ignore_index=True)\n",
                "y_train_full = pd.concat([y_train, y_valid], ignore_index=True)\n",
                "\n",
                "print(f\"\\nФинальное обучение на {X_train_full.shape[0]} образцах...\\n\")\n",
                "\n",
                "finalizer = Finalize(best_estimator)\n",
                "final_accuracy = finalizer.final_score(X_train_full, y_train_full, X_test, y_test)\n",
                "\n",
                "# Create model filename with accuracy\n",
                "model_name_clean = best_model_name.lower().replace(' ', '_')\n",
                "model_filename = f\"work/src/ex04/model/{model_name_clean}_{final_accuracy:.6f}.model\"\n",
                "\n",
                "print(f\"\\nСохранение модели как: {model_filename}\")\n",
                "finalizer.save_model(model_filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Финальный анализ результатов"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 347,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== ФИНАЛЬНЫЙ АНАЛИЗ ===\n",
                        "\n",
                        "🎯 Лучшая модель: Random Forest\n",
                        "📊 Финальная точность: 0.896450\n",
                        "💾 Модель сохранена: work/src/ex04/model/random_forest_0.896450.model\n",
                        "\n",
                        "📈 Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "      Monday       0.83      0.70      0.76        27\n",
                        "     Tuesday       0.92      0.87      0.90        55\n",
                        "   Wednesday       0.90      0.87      0.88        30\n",
                        "    Thursday       0.95      0.96      0.96        80\n",
                        "      Friday       0.95      0.86      0.90        21\n",
                        "    Saturday       0.83      0.89      0.86        54\n",
                        "      Sunday       0.88      0.94      0.91        71\n",
                        "\n",
                        "    accuracy                           0.90       338\n",
                        "   macro avg       0.89      0.87      0.88       338\n",
                        "weighted avg       0.90      0.90      0.90       338\n",
                        "\n",
                        "\n",
                        "🔍 Confusion Matrix:\n",
                        "[[19  2  1  1  0  1  3]\n",
                        " [ 3 48  2  1  0  1  0]\n",
                        " [ 1  1 26  1  0  1  0]\n",
                        " [ 0  1  0 77  0  1  1]\n",
                        " [ 0  0  0  0 18  2  1]\n",
                        " [ 0  0  0  1  1 48  4]\n",
                        " [ 0  0  0  0  0  4 67]]\n",
                        "\n",
                        "❌ Анализ ошибок по дням недели:\n",
                        "Monday: 29.63% ошибок (8/27)\n",
                        "Tuesday: 12.73% ошибок (7/55)\n",
                        "Wednesday: 13.33% ошибок (4/30)\n",
                        "Thursday: 3.75% ошибок (3/80)\n",
                        "Friday: 14.29% ошибок (3/21)\n",
                        "Saturday: 11.11% ошибок (6/54)\n",
                        "Sunday: 5.63% ошибок (4/71)\n",
                        "\n",
                        "🔴 Самый проблемный день: Monday (29.63% ошибок)\n",
                        "🟢 Лучше всего предсказывается: Thursday (3.75% ошибок)\n",
                        "\n",
                        "✅ Pipeline завершен успешно!\n"
                    ]
                }
            ],
            "source": [
                "print(\"=== ФИНАЛЬНЫЙ АНАЛИЗ ===\\n\")\n",
                "print(f\"🎯 Лучшая модель: {best_model_name}\")\n",
                "print(f\"📊 Финальная точность: {final_accuracy:.6f}\")\n",
                "print(f\"💾 Модель сохранена: {model_filename}\")\n",
                "print()\n",
                "\n",
                "# Анализ предсказаний\n",
                "y_pred = best_estimator.predict(X_test)\n",
                "\n",
                "print(\"📈 Classification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=weekdays))\n",
                "\n",
                "print(\"\\n🔍 Confusion Matrix:\")\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "print(cm)\n",
                "\n",
                "# Анализ ошибок по дням недели\n",
                "print(\"\\n❌ Анализ ошибок по дням недели:\")\n",
                "error_rates = {}\n",
                "\n",
                "for i, day in enumerate(weekdays):\n",
                "    if i < len(cm):\n",
                "        total_samples = sum(cm[i, :])\n",
                "        correct_predictions = cm[i, i]\n",
                "        if total_samples > 0:\n",
                "            error_rate = (total_samples - correct_predictions) / total_samples * 100\n",
                "            error_rates[day] = error_rate\n",
                "            print(f\"{day}: {error_rate:.2f}% ошибок ({total_samples - correct_predictions}/{total_samples})\")\n",
                "\n",
                "if error_rates:\n",
                "    worst_day = max(error_rates, key=error_rates.get)\n",
                "    best_day = min(error_rates, key=error_rates.get)\n",
                "    print(f\"\\n🔴 Самый проблемный день: {worst_day} ({error_rates[worst_day]:.2f}% ошибок)\")\n",
                "    print(f\"🟢 Лучше всего предсказывается: {best_day} ({error_rates[best_day]:.2f}% ошибок)\")\n",
                "\n",
                "print(\"\\n✅ Pipeline завершен успешно!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ЗАГРУЗКА МОДЕЛИ И ПРОВЕРКА"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 348,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== ФИНАЛЬНАЯ ПРОВЕРКА: ЗАГРУЗКА И ТЕСТИРОВАНИЕ МОДЕЛИ ===\n",
                        "\n",
                        "Загружаем модель из файла: work/src/ex04/model/random_forest_0.896450.model\n",
                        "✅ Модель успешно загружена из файла\n",
                        "\n",
                        "Используем загруженную модель в классе Finalize...\n",
                        "Применяем к тем же тестовым данным: 338 образцов\n",
                        "\n",
                        "Score загруженной модели: 0.8964497041420119\n",
                        "\n",
                        "=== СРАВНЕНИЕ РЕЗУЛЬТАТОВ ===\n",
                        "\n",
                        "Оригинальный score (до сохранения): 0.8964497041420119\n",
                        "Score загруженной модели:           0.8964497041420119\n",
                        "Разница:                            0.0\n",
                        "\n",
                        "Результаты идентичны: True\n",
                        "\n",
                        "✅ Загруженная модель дает точно такой же результат!\n",
                        "✅ Сохранение и загрузка модели работают корректно\n"
                    ]
                }
            ],
            "source": [
                "print(\"=== ФИНАЛЬНАЯ ПРОВЕРКА: ЗАГРУЗКА И ТЕСТИРОВАНИЕ МОДЕЛИ ===\\n\")\n",
                "\n",
                "# Загружаем сохраненную модель из файла\n",
                "print(f\"Загружаем модель из файла: {model_filename}\")\n",
                "with open(model_filename, 'rb') as f:\n",
                "    loaded_model_final = pickle.load(f)\n",
                "\n",
                "print(\"✅ Модель успешно загружена из файла\")\n",
                "\n",
                "# Создаем новый объект Finalize с загруженной моделью\n",
                "loaded_finalizer_test = Finalize(loaded_model_final)\n",
                "\n",
                "print(f\"\\nИспользуем загруженную модель в классе Finalize...\")\n",
                "print(f\"Применяем к тем же тестовым данным: {X_test.shape[0]} образцов\\n\")\n",
                "\n",
                "# Модель уже обучена, поэтому мы НЕ вызываем fit()\n",
                "# Получаем предсказания и score\n",
                "loaded_predictions = loaded_model_final.predict(X_test)\n",
                "loaded_final_score = accuracy_score(y_test, loaded_predictions)\n",
                "\n",
                "print(f\"Score загруженной модели: {loaded_final_score}\")\n",
                "\n",
                "# Сравниваем с оригинальным результатом\n",
                "print(f\"\\n=== СРАВНЕНИЕ РЕЗУЛЬТАТОВ ===\\n\")\n",
                "print(f\"Оригинальный score (до сохранения): {final_accuracy}\")\n",
                "print(f\"Score загруженной модели:           {loaded_final_score}\")\n",
                "print(f\"Разница:                            {abs(final_accuracy - loaded_final_score)}\")\n",
                "\n",
                "scores_identical = abs(final_accuracy - loaded_final_score) < 1e-15\n",
                "print(f\"\\nРезультаты идентичны: {scores_identical}\")\n",
                "\n",
                "if scores_identical:\n",
                "    print(\"\\n✅ Загруженная модель дает точно такой же результат!\")\n",
                "    print(\"✅ Сохранение и загрузка модели работают корректно\")\n",
                "else:\n",
                "    print(\"\\n❌ ОШИБКА: Результаты не совпадают!\")\n",
                "    print(\"❌ Проблема с сохранением/загрузкой модели\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
